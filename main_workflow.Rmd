---
title: "R Notebook"
output: html_notebook
author:
- Komal Marathe
---

### IMP NOTE

**Please read README.md file before attempting to run this Rmd notebook.**

```{r}
library(tidyverse)
library(dada2)
```


### Store relevant paths that will be used in the script
```{r}
# Path to Silva database
silva_db_path <- "./db/silva_nr99_v138.1_train_set.fa.gz"

# Path to the folder where all the input fastq files are stored
path <- "./input"
```


### DADA2 Part
```{r}
# List all the files present in `path` variable
list.files(path)

#Sort files to ensure forward/reverse are in the same order
fnFs <- sort(list.files(path, pattern="_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq.gz", full.names = TRUE))

#extract sample names, assuming filenames have format:
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

#Now we visualize the quality profile of the reverse reads:
plotQualityProfile(fnFs[3:6])
plotQualityProfile(fnRs[3:6])

# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

# Get the total number of input fastq files
length(fnFs)
length(fnRs)

# Check if there are any duplicate input fastq files
any(duplicated(c(fnFs, fnRs)))
any(duplicated(c(filtFs, filtRs)))

# Check of total number of filtered fastq files
length(filtFs)
length(filtRs)

#Trim based on quality plots, in this example we cut for the forward reads to 240bp and the reverse reads to 200bp
out<- filterAndTrim(fnFs, 
                    filtFs, 
                    fnRs, 
                    filtRs, 
                    maxN=0, 
                    truncLen=c(240,200),
                    maxEE=c(2,2), 
                    truncQ=2, 
                    rm.phix=TRUE, 
                    compress=TRUE, 
                    multithread=TRUE)  # On Windows set multithread=FALSE
head(out)

#Plot again to see if the trimming worked
plotQualityProfile(filtFs[3:6])
plotQualityProfile(filtRs[3:6])

table(file.exists(filtFs))
table(file.exists(filtRs))

exists <- file.exists(filtFs) & file.exists(filtRs)
filtFs <- filtFs[exists]
filtRs <- filtRs[exists]

#Learn the Error Rates
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
# Plot error model
plotErrors(errF, nominalQ=TRUE)

#Apply the core sample inference algorithm to the filtered and trimmed sequence data.
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)

#Inspecting the returned dada-class object:
dadaFs[[1]]
dadaRs[[1]]

#Merge paires reads
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[3]])
head(mergers[[10]])
#Construct sequence table to ASV
seqtab <- makeSequenceTable(mergers)
dim(seqtab)


# Inspect distribution of sequence lengths
# the amplicon should be around 299 to 303 bp long
table(nchar(getSequences(seqtab)))
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 299:303]
table(nchar(getSequences(seqtab2)))

# Write seqtab to a csv file which will be used in python script
write.csv(t(seqtab2), file.path("./output/input_seqtab.csv"))
```


### Python Part

This next code chunk, is to be run on command line. DO NOT run it here in Rstudio, it does not work via Rstudio.

The purpose of this python part is to run the python script named `process_dada2_seqtab.py`.

This python script identifies well for each sequence based on its barcodes, removes barcodes and primers and keeps the original sequence (ASV), and keeps the counts of sequences as-is.

Please read the following explanation of the python command you will need to run as part of this section.

**process_dada2_seqtab.py** is the python script that processes sequences.
--fprimer takes forward primer sequence.
--rprimer takes reverse primer sequence.
--seqtab-path takes the path of the **input_seqtab.csv** file we get from dada2 workflow above.
--barcode-path takes the path to the csv file containing barcode information.
--output-path takes the name of the output file. Keep it as **processed_data_output.csv**

Now that you have read above explanation of what happens with this python section, please copy the following python command, open the "Terminal" tab at the bottom window of your Rstudio, and run this python command there. Remember to run this on "Terminal" tab instead of "Console" tab.

```{}
python3 process_dada2_seqtab.py --fprimer GTGCCAGCMGCCGCGGTAA \
--rprimer GACTACHVGGGTATCTAATCC \
--seqtab-path ./output/input_seqtab.csv \
--barcode-path BC_to_well2.csv \
--output-path ./output/processed_data_output.csv
```

Once you have successfully run this python command, then proceed to executing subsequent/below code chunks directly in Rstudio as done previously.

### Collapse counts for unique sequencess
```{r}
# Read processed data file
processed_data <- read.csv("./output/processed_data_output.csv")
# Extract trimmed seqs and their count information
trimmed_seqs_data <- processed_data %>% select("trimmed_seq", starts_with("Plate"))

# Collapse (sum) plate-well values for each unique trimmed 
unique_trimmed_seqs <- aggregate(. ~ trimmed_seq, data=trimmed_seqs_data, FUN=sum)

rownames(unique_trimmed_seqs) <- unique_trimmed_seqs$trimmed_seq
unique_trimmed_seqs$trimmed_seq <- NULL

# write.csv(unique_trimmed_seqs, file="unique_trimmed_seqs_data.csv")
```


### Purity Calculation
```{r}
source("filter_purity.R")
```

The process_each_sequence function used in the next code chunk takes two arguments.
1. dataframe of trimmed unique sequences and their counts.
2. top n highest counts for each sequences. If you would like to get all the counts in descending order,
input the total number of well-plate combinations. In this case there were 960 well-plate combos.
Here we used 1100. Even though there were 960 well-plate combos, it will still print all the 960 counts for each sequence.
If you do not know the exact number of total number of well-plate combinations, and provide a number greater than that,
you will still give you all the counts.


The get_taxonomy function used in the next code chunk  takes two arguments.
1. A vector of all the trimmed unique sequences (ASVs).
2. Path to the silva database file
This function uses the assignTaxonomy function from dada2 package in the background.

```{r}
final_df <- process_each_sequence(unique_trimmed_seqs, 1100)
tax <- get_taxonomy(final_df$ASV, silva_db_path)

fd <- final_df
rownames(fd) <- fd$ASV
fd_with_taxa <- merge(tax, fd, by = 0, all = TRUE)
names(fd_with_taxa)[names(fd_with_taxa) == "Row.names"] <- "ASV"

# Write final data frame into a csv file
# Uncomment following line if you would like to store the final result in a csv
# write.csv(fd_with_taxa, file = "./output/asv_analysis_results.csv")
```



