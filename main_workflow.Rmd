---
title: "R Notebook"
output: html_notebook
---


```{r}
library(tidyverse)
library(dada2)
library(reticulate)
```


### Store relevant paths that will be used in the script
```{r}
# Path to Silva database
silva_db_path <- "~/Documents/silva_db/silva_nr99_v138.1_train_set.fa.gz"

# Path to the folder where all the input fastq files are stored
path <-"input_data/fastq_files_egla"
```


### DADA2 Part
```{r}
path <-"input_data/fastq_files_egla"
list.files(path)

#Sort files to ensure forward/reverse are in the same order
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))

#extract sample names, assuming filenames have format:
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

#Now we visualize the quality profile of the reverse reads:
plotQualityProfile(fnRs[5:6])
plotQualityProfile(fnRs[10:11])

# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

# Get the total number of input fastq files
length(fnFs)
length(fnRs)

# Check if there are any duplicate input fastq files
any(duplicated(c(fnFs, fnRs)))
any(duplicated(c(filtFs, filtRs)))

# Check of total number of filtered fastq files
length(filtFs)
length(filtRs)

#Trim based on quality plots and trimming primers 19bp for F and 21bp R
out<- filterAndTrim(fnFs, 
                    filtFs, 
                    fnRs, 
                    filtRs, 
                    maxN=0, 
                    maxEE=c(2,2), 
                    truncQ=2, 
                    rm.phix=TRUE, 
                    compress=TRUE, 
                    multithread=TRUE)  # On Windows set multithread=FALSE
head(out)

#Plot again to see if the trimming worked
plotQualityProfile(filtFs[1:2])
plotQualityProfile(filtRs[1:2])

table(file.exists(filtFs))
table(file.exists(filtRs))

exists <- file.exists(filtFs) & file.exists(filtRs)
filtFs <- filtFs[exists]
filtRs <- filtRs[exists]

#Learn the Error Rates
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
# Plot error model
plotErrors(errF, nominalQ=TRUE)

#Apply the core sample inference algorithm to the filtered and trimmed sequence data.
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)

#Inspecting the returned dada-class object:
dadaFs[[1]]
dadaRs[[1]]

#Merge paires reads
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[3]])
head(mergers[[10]])
#Construct sequence table to ASV
seqtab <- makeSequenceTable(mergers)
dim(seqtab)


# Inspect distribution of sequence lengths
#must be 251
table(nchar(getSequences(seqtab)))
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 299:303]
table(nchar(getSequences(seqtab2)))

# Write seqtab to a csv file which will be used in python script
write.csv(t(seqtab2), file.path(path, "input_seqtab.csv"))
```


### Python Part
```{r setup, include=FALSE}
python_path <- Sys.which("python3")
# use_python(python_path, required = T)
use_virtualenv("/Users/komalmarathe/Documents/test_scripts/test_well_plate_project/python_env")
```

F_PRIMER = "GTGCCAGCMGCCGCGGTAA"
R_PRIMER = "GACTACHVGGGTATCTAATCC"
args.seqtab_path = "./input_seqtab.csv"
args.barcode_path = "BC_to_well2.csv"
args.output_path = "processed_data_output.csv"

```{bash}
python3 process_dada2_seqtab.py --fprimer GTGCCAGCMGCCGCGGTAA \
--rprimer GACTACHVGGGTATCTAATCC \
--seqtab-path ./input_seqtab.csv \
--barcode-path BC_to_well2.csv \
--output-path processed_data_output.csv
```


### Collapse counts for unique sequencess
```{r}
# Read processed data file
processed_data <- read.csv("processed_data_output.csv")
# Extract trimmed seqs and their count information
trimmed_seqs_data <- processed_data %>% select("trimmed_seq", starts_with("Plate"))

# Collapse (sum) plate-well values for each unique trimmed 
unique_trimmed_seqs <- aggregate(. ~ trimmed_seq, data=trimmed_seqs_data, FUN=sum)

rownames(unique_trimmed_seqs) <- unique_trimmed_seqs$trimmed_seq
unique_trimmed_seqs$trimmed_seq <- NULL

# write.csv(unique_trimmed_seqs, file="unique_trimmed_seqs_data.csv")
```


### Purity Calculation
```{r}
source("filter_purity.R")
```


```{r}
final_df <- process_each_sequence(unique_trimmed_seqs, 1100)
tax <- get_taxonomy(final_df$ASV, silva_db_path)

fd <- final_df
rownames(fd) <- fd$ASV
fd_with_taxa <- merge(tax, fd, by = 0, all = TRUE)
names(fd_with_taxa)[names(fd_with_taxa) == "Row.names"] <- "ASV"

# Write final data frame into a csv file
# write.csv(fd_with_taxa, file = "asv_analysis_results.csv")
```



